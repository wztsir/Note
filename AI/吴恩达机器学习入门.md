

## 疑问

神经网络增加层数和增加没层神经元个数的区别？

## Jupyter notebook

进入命令模式之后（此时你没有活跃单元），有以下快捷键：

- `A`：在所选单元之上插入一个新的单元
- `B`：在所选单元之下插入一个新的单元
- `D`：连续按两次删除所选的单元
- `Z`：撤销被删除的单元
- `Y`：将当前选中的单元变成一个代码单元
- `F`：查找和替换
- `Shift +上或下箭头`：可选择多个单元。
- `Shift + M`：在多选模式时，可合并你的选择。

处于编辑模式时（在命令模式时按 Enter 会进入编辑模式），下列快捷键很有用：

Ctrl + Home ：到达单元起始位置
Ctrl + S ：保存进度
Ctrl + Enter ：会运行你的整个单元块
Alt + Enter ：不止会运行你的单元块，还会在下面添加一个新单元
Ctrl + Shift + F ：打开命令面板



## 线性回归

由于不断下降，随着越来越接近最低点，导数会变得更小，每次下降的幅度也会大幅度减小，也就是导数即将到0的时候幅度会很小，也就很容易的达到0，参数就可以刚好到达局部最小值点，而非超过那个点



矢量化运行更快，因为借助了电脑上平行的硬件

### 特称工程

用已有的特征，构造生成新的特征



### 代码

Training data is 𝑚m examples by 𝑛n features creating an (m,n) array. 

-1代表默认的数据，可以通过计算自动得到

```
a = np.arange(6).reshape(-1, 2) 	
```



### 数据格式化

`%`运算符就是用来格式化字符串的。在字符串内部，`%s`表示用字符串替换，`%d`表示用整数替换，有几个`%?`占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个`%?`，括号可以省略。



| 占位符 | 替换内容     |
| :----- | :----------- |
| %d     | 整数         |
| %f     | 浮点数       |
| %s     | 字符串       |
| %x     | 十六进制整数 |

三种数据格式化

```python
s1 = 72
s2 = 85
r = (s2 - s1) / s1 * 100  # 从坐左往右计算
print('数据的增长率为%.1f%%' % r)
print('数据的增长率为{0:.3f}%'.format(r))
print(f'数据的增长率为{r:.5f}%')
```

```python
print('数据的增长率为%10.1f%%' % r)
print('数据的增长率为{0:10.3f}%'.format(r))
print(f'数据的增长率为{r:10.5f}%')
```

.左边的是数据总体长度的占位符



### 逻辑回归

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220916201411217.png" alt="image-20220916201411217" style="zoom:50%;" />

实在是太漂亮了，调整不同的参数拟合不同的曲线

sigmoid function



### 归一化

不同特征需要统一数量级

不然梯度下降过程中需要设置的梯度前的系数不一样，特征数量级大的，梯度更大，但参数不一定更大，变化的可能会更大



### 随机梯度下降（sgd）

stochastic gradient descent



## 分类问题

### 损失函数

#### 浅显认知

损失函数是用来修正模型的，当预测结果与真实值相差很大时，损失函数应当很大，当损失函数与真实值相差不大时，损失函数应当不大

所以，分类问题，当存在0.7概率预测为1，与0.6概率预测为1

真实值为0时的损失函数情况，如下：

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220908211952052.png" alt="image-20220908211952052" style="zoom:50%;" />



之前线性回归用的损失函数现在用不了，因为不是凸函数，导数存在很多为0的点

还和最大似然估计有关

#### 更深度认知

损失函数影响着优化的进行，用平方和会导致在数据极端的情况下变化不大



最大化可能性

最小化交叉熵



### 过拟合问题解决

找更多数据集

减少特征数目

正则化，而不是一刀将特征参数砍为0





### 交叉熵损失函数

#### 信息量

针对一个时间，发生的概率越小，信息量越大，发生的概率越大，信息量越小

比如太阳明天从东边升起，概率为1，信息量为0

#### 相对熵

两个概率分布的差距，差值越小，两个概率分布越接近

![相对熵](D:\Note\AI\相对熵.png)

#### 交叉熵

分类问题中，将P（xi）为实际标签值，已知，在不断调整的是交叉熵

所以从相对熵越小，即交叉熵越小，公式如下：

![交叉熵](D:\Note\AI\交叉熵.png)

为了计算方便，损失函数变为交叉熵

## 多层感知机

Multilayer Perceptron

 增加位置(13,17)处像素的强度是否总是增加（或降低）图像描绘狗的似然？ 对线性模型的依赖对应于一个隐含的假设， 即区分猫和狗的唯一要求是评估单个像素的强度。 在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。

这里的线性很荒谬， 而且我们难以通过简单的预处理来解决这个问题。 这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。 在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。

激活函数的选择

1、连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参           数。

2、激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。

3、激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定



## 神经网络

符号说明

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220912135126608.png" alt="image-20220912135126608" style="zoom: 50%;" />





## 常见的包

### numpy

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220912212328846.png" alt="image-20220912212328846" style="zoom: 67%;" />

最后一个是向量而不是举证





代码讲解

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220913143952082.png" alt="image-20220913143952082" style="zoom:50%;" />





## overfitting

先用简单的模型测试优化有没有大致问题，再选择弹性比较大的模型，更深的模型去尝试，如果效果反而更低，则是优化的问题

训练的loss的损失比较小，但测试的loss却比较大



真正的orverfitting，过度拟合

- 获得更多的数据
- 做数据增强
- 让模型不要那么有弹性



## 数据处理

```
x=x+y;
```

上面的代码开辟新的内存，会导致内存消耗过大。

下面的代码，x的id不变，id可以理解为指针，即内存空间不变

```
x[:]=x+y;
x+=y;
```





reshape:是浅拷贝



## 矩阵求导

关键在于布局，是分子布局还是分母布局

如下为分子布局 

![image-20220920103726083](C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920103726083.png)

例子：标量/向量—分子布局

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920103139591.png" alt="image-20220920103139591" style="zoom:50%;" />

记忆—常用：||X||^2  求导是2X^T  是因为分子布局



向量/向量—R(m)/R(n)  =>  RmXn

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920122535456.png" alt="image-20220920122535456" style="zoom:50%;" />

​					向量/向量常用

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920124051173.png" alt="image-20220920124051173" style="zoom:33%;" />

**A**是矩阵，可以推导，简单				

理解 **y=x**  时求导为单位矩阵

**y=AX**    、**y=X*T A**   都很常用，是重要的，需要记住



更一般的情况

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920123037917.png" alt="image-20220920123037917" style="zoom: 33%;" />





典型例子

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920110054087.png" alt="image-20220920110054087" style="zoom:50%;" />

向量默认为列向量

向量的内积，即两个列向量相乘，最后求导即标量对列向量求导，结果是行向量



典型例子

<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20220920123214088.png" alt="image-20220920123214088" style="zoom:50%;" />





正向传播与反向传播

反向传播从根节点到叶子节点，需要储存每一个节点的值，在计算叶子节点的时候，每一个叶子节点都可以直接用到之前存储了的值

正向传播，从叶子节点到根节点不存储中间节点的值，无法传播到所有叶子



## API

### python内置

zip函数

按照最小列表的长度，将两个列表打包成合并成元组



### torch

自动计算梯度



```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()

y.backward()
x.grad
```

调用X的函数进行反向传播

然后展示梯度



#### 计算

(dim=0,就是第一个维度，理解为形状消除这个维度)

cumsum：按照维度累加，不改变形状

keepdim：将特定维度消除，但没有消失，改变形状，变为1

```pyhon
cum_counts = counts.cumsum(dim=0)
temp = cum_counts.sum(dim=1, keepdim=True)
estimates = cum_counts / temp
```





#### 全连接层

```
nn.Linear(2, 1)
// 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。
```

内置了参数，帮助数据进行形状变换

数学公式

s形生长曲线

```python
.sigmoid()
```



#### 网络

构建网络后，输入输入值，会自动调用前向传播forward

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        z1=torch.mm(X, self.rand_weight)
        z=torch.mm(X, self.rand_weight) + 1
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        
        while X.abs().sum() > 1:
            y = X.abs().sum()
            X /= 2
        return X.sum()


net2 = FixedHiddenMLP()
print(net2(X))
```





#### nn

平坦化，将一个tensor连续的几个维度展平一个维度

```
nn.Flatten(),
```



### matplotlib

画图需要使用的包

直接看别人写的代码，什么不懂就删除什么，或者修改什么，就知道是某个参数是用来干什么的了

```pyhon
# 1.准备数据
x = range(60)
y = [random.uniform(15, 20) for i in x]

# 2.创建画布
plt.figure(figsize=(12, 8), dpi=80)
plt.plot(x, y)
print(y)
# 3 设置刻度及步长
z = range(40)
x_label = ['11:{}'.format(i) for i in x]
# 设置光标并且设置每个光标对应的label
plt.xticks(x[::5], x_label[::5])
plt.yticks(z[::5])  # 5是步长

# 4 添加网格信息
plt.grid(True, linestyle='--', alpha=0.5)  # 默认是True，风格设置为虚线，alpha为透明度

# 5 添加标题（中文在plt中默认乱码，不乱码的方法在本文最后说明）
plt.xlabel('Time')
plt.ylabel('Temperature')
plt.title('Curve of Temperature Change with Time')

# 6 保存图片，并展示
# plt.savefig('./plt_png/test1.2.png')
plt.show()
```



